---
tags:
  - Enterprise Option
displayed_sidebar: docsJapanese
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import TranslationBanner from "/src/components/_translation-ja-jp.mdx";

# ScalarDB Analytics を通じた分析クエリの実行

<TranslationBanner />

このガイドでは、ScalarDB Analytics アプリケーションの開発方法について説明します。アーキテクチャと設計の詳細については、[ScalarDB Analytics の設計](./design.mdx)を参照してください。

ScalarDB Analytics は現在、実行エンジンとして Spark を使用し、Spark カスタムカタログプラグインを提供することによって、ScalarDB で管理されているデータソースと管理されていないデータソースの統合ビューを Spark テーブルとして提供します。これにより、任意の Spark SQL クエリをシームレスに実行できます。

## 準備

このセクションでは、前提条件、ScalarDB Analytics セットアップのための Spark 設定、および ScalarDB Analytics 依存関係の追加について説明します。

### 前提条件

- **ScalarDB Analytics server:** カタログメタデータを管理し、データソースに接続する実行中のインスタンス。サーバーには少なくとも1つのデータソースが登録されている必要があります。データソースの登録については、[ScalarDB Analytics カタログの作成](./create-scalardb-analytics-catalog.mdx)を参照してください。
- **Apache Spark:** 互換性のあるバージョンの Apache Spark。サポートされているバージョンについては、[Spark](../requirements.mdx#spark)を参照してください。まだ Spark をインストールしていない場合は、[Apache Spark のウェブサイト](https://spark.apache.org/downloads.html)から Spark ディストリビューションをダウンロードしてください。

### ScalarDB Analytics のセットアップのための Spark 設定

ScalarDB Analytics は ScalarDB Analytics server と統合するために特定の Spark 設定が必要です。

#### 必要な Spark 設定

ScalarDB Analytics を Spark で使用するには、以下を設定する必要があります:

1. **ScalarDB Analytics パッケージ:** Spark と Scala のバージョンに一致する JAR 依存関係を追加
2. **メータリングリスナー:** 課金のためのリソース使用状況を追跡するリスナーを登録
3. **カタログ登録:** ScalarDB Analytics サーバーに接続する Spark カタログを登録

Spark を設定する際は、ScalarDB Analytics サーバー上で作成されたカタログと一致するカタログ名を指定する必要があります。これにより、Spark がそのカタログで管理されているデータソースに正しくアクセスできるようになります。

#### 設定例

以下は完全な設定例です:

```conf
# 1. ScalarDB Analytics パッケージ
spark.jars.packages com.scalar-labs:scalardb-analytics-spark-all-<SPARK_VERSION>_<SCALA_VERSION>:<SCALARDB_ANALYTICS_VERSION>

# 2. メータリングリスナー
spark.extraListeners com.scalar.db.analytics.spark.metering.ScalarDbAnalyticsListener

# 3. カタログ登録
spark.sql.catalog.myanalytics com.scalar.db.analytics.spark.ScalarDBAnalyticsCatalog
spark.sql.catalog.myanalytics.server.host analytics-server.example.com
spark.sql.catalog.myanalytics.server.catalog.port 11051
spark.sql.catalog.myanalytics.server.metering.port 11052
```

プレースホルダーを置き換えてください:

- `<SPARK_VERSION>`: 使用している Spark のバージョン (例: `3.5` または `3.4`)
- `<SCALA_VERSION>`: 使用している Scala のバージョン (例: `2.13` または `2.12`)
- `<SCALARDB_ANALYTICS_VERSION>`: ScalarDB Analytics のバージョン (例: `3.17.0`)

この例では:

- カタログ名 `myanalytics` は、ScalarDB Analytics サーバー上に存在するカタログと一致する必要があります.
- ScalarDB Analytics サーバーは `analytics-server.example.com` で実行されています.
- テーブルには `myanalytics.<data_source>.<namespace>.<table>` の形式でアクセスします.

:::important

Spark 設定のカタログ名は、CLI を使用して ScalarDB Analytics サーバー上で作成されたカタログの名前と一致させる必要があります。たとえば、サーバー上で `production` という名前のカタログを作成した場合、Spark 設定プロパティでカタログ名として `production` を使用する必要があります (例: `spark.sql.catalog.production`、`spark.sql.catalog.production.server.host` など)。

:::

:::note

データソース設定は ScalarDB Analytics server で管理されます。ScalarDB Analytics server でのデータソースの設定方法については、[ScalarDB Analytics カタログの作成](./create-scalardb-analytics-catalog.mdx)を参照してください。

:::

### Spark アプリケーションのビルド設定

ScalarDB Analytics を使用する Spark アプリケーションを開発する際は、ビルド設定に依存関係を追加できます。たとえば Gradle の場合:

```kotlin
dependencies {
    implementation("com.scalar-labs:scalardb-analytics-spark-all-<SPARK_VERSION>_<SCALA_VERSION>:<SCALARDB_ANALYTICS_VERSION>")
}
```

:::note

Gradle Shadow や Maven Shade などのプラグインを使用してアプリケーションを fat JAR にバンドルする場合は、`provided` や `shadow` などの設定を使用して fat JAR から ScalarDB Analytics を除外してください。

:::

## Spark アプリケーションの開発

このセクションでは、Java を使用して ScalarDB Analytics を使用する Spark アプリケーションを開発する方法について説明します。

ScalarDB Analytics を使用した Spark アプリケーションの開発には3つの方法があります:

1. **Spark ドライバーアプリケーション:** クラスター内で実行される従来のSparkアプリケーション
2. **Spark Connect アプリケーション:** Spark Connectプロトコルを使用するリモートアプリケーション
3. **JDBC アプリケーション:** JDBCインターフェースを使用するリモートアプリケーション

:::note

環境によっては、上記のすべての方法を使用できない場合があります。サポートされる機能とデプロイメントオプションの詳細については、[サポートされているマネージド Spark サービスとそのアプリケーションタイプ](./deployment.mdx#サポートされているマネージド-spark-サービスとそのアプリケーションタイプ)を参照してください。

:::

これらのすべての方法で、同じテーブル識別子形式を使用して ScalarDB Analytics のテーブルを参照できます。ScalarDB Analytics がデータソースからカタログ情報をマッピングする方法の詳細については、[カタログ情報リファレンス](./design.mdx#データソース別のカタログ情報マッピング)を参照してください。

<Tabs groupId="spark-application-type" queryString>
  <TabItem value="spark-driver" label="Spark ドライバーアプリケーション">

ScalarDB Analytics には一般的に使用される `SparkSession` クラスを使用できます。また、YARN、Kubernetes、スタンドアロン、ローカルモードなど、Spark がサポートするあらゆるタイプのクラスターデプロイメントを使用できます。

ScalarDB Analytics のテーブルからデータを読み取るには、通常のSparkテーブルを読み取る場合と同じように `spark.sql` または `spark.read.table` 関数を使用できます。

まず、Java プロジェクトをセットアップする必要があります。例えば、Gradle を使用している場合は、`build.gradle.kts` ファイルに以下を追加できます:

```kotlin
dependencies {
    implementation("com.scalar-labs:scalardb-analytics-spark-all-<SPARK_VERSION>_<SCALA_VERSION>:<SCALARDB_ANALYTICS_VERSION>")
}
```

以下は Spark ドライバーアプリケーションの例です:

```java
import org.apache.spark.sql.SparkSession;

public class MyApp {
    public static void main(String[] args) {
        // SparkSessionを作成
        try (SparkSession spark = SparkSession.builder().getOrCreate()) {
            // ScalarDB Analyticsのテーブルからデータを読み取る
            spark.sql("SELECT * FROM my_catalog.my_data_source.my_namespace.my_table").show();
        }
    }
}
```

その後、`spark-submit` コマンドを使用してアプリケーションをビルドして実行できます。

:::note

通常の Spark アプリケーションと同様に、アプリケーションの fat JAR ファイルをビルドする必要がある場合があります。

:::

```console
spark-submit --class MyApp --master local[*] my-spark-application-all.jar
```

:::tip

`spark-sql` や `spark-shell` などの Spark が提供する他の CLI ツールを使用して、ScalarDB Analytics のテーブルを操作することもできます。

:::

  </TabItem>
  <TabItem value="spark-connect" label="Spark Connect アプリケーション">

[Spark Connect](https://spark.apache.org/spark-connect/) を使用して ScalarDB Analytics と対話できます。Spark Connect を使用することで、リモートの Spark クラスターにアクセスし、Spark ドライバーアプリケーションと同じ方法でデータを読み取ることができます。以下では、Spark Connect の使用方法について簡単に説明します。

まず、リモートの Spark クラスターで以下のコマンドを実行して、Spark Connect サーバーを起動する必要があります:

```console
./sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_<SCALA_VERSION>:<SPARK_FULL_VERSION>,com.scalar-labs:scalardb-analytics-spark-all-<SPARK_VERSION>_<SCALA_VERSION>:<SCALARDB_ANALYTICS_VERSION>
```

括弧内の内容は以下のように変更する必要があります:

- `<SCALA_VERSION>`: Spark インストールに対応する Scala のメジャーおよびマイナーバージョン (例: 2.12 または 2.13)
- `<SPARK_FULL_VERSION>`: 使用している Spark の完全なバージョン (例: 3.5.3)
- `<SPARK_VERSION>`: 使用している Spark のメジャーおよびマイナーバージョン (例 3.5)
- `<SCALARDB_ANALYTICS_VERSION>`: ScalarDB Analytics のバージョン

:::note

パッケージのバージョンは、使用している Spark と ScalarDB Analytics のバージョンと一致する必要があります。

:::

また、アプリケーションに Spark Connect クライアントパッケージを含める必要があります。例えば、Gradle を使用している場合は、`build.gradle.kts` ファイルに以下を追加できます:

```kotlin
implementation("org.apache.spark:spark-connect-client-jvm_2.12:3.5.3")
```

その後、Spark Connect クライアントアプリケーションを作成してサーバーに接続し、データを読み取ることができます。

```java
import org.apache.spark.sql.SparkSession;

public class MyApp {
    public static void main(String[] args) {
        try (SparkSession spark = SparkSession.builder()
            .remote("sc://<CONNECT_SERVER_URL>:<CONNECT_SERVER_PORT>")
            .getOrCreate()) {

            // ScalarDB Analyticsのテーブルからデータを読み取る
            spark.sql("SELECT * FROM my_catalog.my_data_source.my_namespace.my_table").show();
        }
    }
}
```

以下のコマンドを実行して、Spark Connect クライアントアプリケーションを通常の Java アプリケーションとして実行できます:

```console
java -jar my-spark-connect-client.jar
```

Spark Connect の使用方法の詳細については、[Spark Connect のドキュメント](https://spark.apache.org/docs/latest/spark-connect-overview.html)を参照してください。

  </TabItem>
  <TabItem value="jdbc" label="JDBC アプリケーション">

残念ながら、Spark Thrift JDBC サーバーは ScalarDB Analytics に必要な Spark の機能をサポートしていないため、Apache Spark 環境でJDBCを使用して ScalarDB Analytics からデータを読み取ることはできません。JDBC アプリケーションについてここで言及しているのは、一部のマネージド Spark サービスが JDBC インターフェースを介して Spark クラスターと対話する異なる方法を提供しているためです。詳細については、[サポートされているマネージド Spark サービスとそのアプリケーションタイプ](./deployment.mdx#サポートされているマネージド-spark-サービスとそのアプリケーションタイプ)を参照してください。

  </TabItem>
</Tabs>

## カタログ情報のマッピング

ScalarDB Analytics は、データソース、名前空間、テーブル、列を含む独自のカタログを管理します。この情報は自動的に Spark カタログにマッピングされます。このセクションでは、ScalarDB Analytics がカタログ情報を Spark カタログにマッピングする方法について説明します。

データソース内の情報が ScalarDB Analytics カタログにマッピングされる方法の詳細については、[データソース別のカタログ構造マッピング](./reference-data-source.mdx#データソース別のカタログ構造マッピング)を参照してください。

### カタログ構造のマッピング

ScalarDB Analytics は、データソースからのカタログ構造を Spark カタログにマッピングします。ScalarDB Analytics カタログのデータソースのテーブルは、Spark テーブルにマッピングされ、以下の形式で識別されます:

```console
<CATALOG_NAME>.<DATA_SOURCE_NAME>.<NAMESPACE_NAMES>.<TABLE_NAME>
```

括弧内の内容は以下の通りです:

- `<CATALOG_NAME>`: カタログの名前
- `<DATA_SOURCE_NAME>`: データソースの名前
- `<NAMESPACE_NAMES>`: 名前空間の名前。名前空間が複数レベルある場合は、ドット (`.`) で区切って連結されます
- `<TABLE_NAME>`: テーブルの名前

例えば、`my_catalog` という名前の ScalarDB カタログに、`my_data_source` という名前のデータソースと `my_schema` という名前のスキーマがある場合、そのスキーマ内の `my_table` という名前のテーブルを `my_catalog.my_data_source.my_schema.my_table` として参照できます。

### データ型マッピング

ScalarDB Analytics は、カタログ内のデータ型を Spark データ型にマッピングします。以下の表は、データ型がどのようにマッピングされるかを示しています:

| ScalarDB データ型 | Spark データ型     |
| :---------------- | :----------------- |
| `BYTE`            | `Byte`             |
| `SMALLINT`        | `Short`            |
| `INT`             | `Integer`          |
| `BIGINT`          | `Long`             |
| `FLOAT`           | `Float`            |
| `DOUBLE`          | `Double`           |
| `DECIMAL`         | `Decimal`          |
| `TEXT`            | `String`           |
| `BLOB`            | `Binary`           |
| `BOOLEAN`         | `Boolean`          |
| `DATE`            | `Date`             |
| `TIME`            | `TimestampNTZ`     |
| `TIMESTAMP`       | `TimestampNTZ`     |
| `TIMESTAMPTZ`     | `Timestamp`        |
| `DURATION`        | `CalendarInterval` |
| `INTERVAL`        | `CalendarInterval` |

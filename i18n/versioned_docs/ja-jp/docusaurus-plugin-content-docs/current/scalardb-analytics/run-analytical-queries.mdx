---
tags:
  - Enterprise Option
displayed_sidebar: docsJapanese
---

# ScalarDB Analytics を通じた分析クエリの実行

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import TranslationBanner from '/src/components/_translation-ja-jp.mdx';

<TranslationBanner />

このガイドでは、ScalarDB Analytics アプリケーションの開発方法について説明します。アーキテクチャと設計の詳細については、[ScalarDB Analytics の設計](./design.mdx)を参照してください。

ScalarDB Analytics は現在、実行エンジンとして Spark を使用し、Spark カスタムカタログプラグインを提供することによって、ScalarDB で管理されているデータソースと管理されていないデータソースの統合ビューを Spark テーブルとして提供します。これにより、任意の Spark SQL クエリをシームレスに実行できます。

## 準備

このセクションでは、前提条件、ScalarDB Analytics セットアップのための Spark 設定、および ScalarDB Analytics 依存関係の追加について説明します。

### 前提条件

ScalarDB Analytics は Apache Spark 3.4以降で動作します。まだ Spark をインストールしていない場合は、[Apache Spark のウェブサイト](https://spark.apache.org/downloads.html)から Spark ディストリビューションをダウンロードしてください。

:::note

Apache Spark は Scala 2.12 または Scala 2.13 でビルドされています。ScalarDB Analytics は両方のバージョンをサポートしています。後で適切なバージョンの ScalarDB Analytics を選択できるように、使用しているバージョンを確認してください。詳細については、[バージョン互換性](#バージョン互換性)を参照してください。

:::

### ScalarDB Analytics のセットアップのための Spark 設定

以下のセクションでは、ScalarDB Analyticsで利用可能なすべての設定オプションについて説明します。

- ScalarDB AnalyticsのSparkとの統合方法
- データソースの接続とアクセス方法
- ライセンス情報の提供方法

実践的なシナリオでの設定例については、[サンプルアプリケーション設定](../scalardb-samples/scalardb-analytics-spark-sample/README.mdx#scalardb-analytics-の設定)を参照してください。

#### Spark プラグインの設定

| 設定キー名                           | 必須   | 説明                                                                                                                                                                                                                                                                                                |
|:-----------------------------------|:------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `spark.jars.packages`              | いいえ | 必要な依存関係の Maven 座標をカンマ区切りで指定します。使用する ScalarDB Analytics パッケージを含める必要があります。含めない場合は、Spark アプリケーションの実行時にコマンドライン引数として指定します。ScalarDB Analytics の Maven 座標の詳細については、[ScalarDB Analytics 依存関係の追加](#scalardb-analytics-依存関係の追加)を参照してください。 |
| `spark.sql.extensions`             | はい   | `com.scalar.db.analytics.spark.Extensions` を設定する必要があります。                                                                                                                                                                                                                                    |
| `spark.sql.catalog.<CATALOG_NAME>` | はい   | `com.scalar.db.analytics.spark.ScalarCatalog` を設定する必要があります。           

`<CATALOG_NAME>`には任意の名前を指定できます。設定全体で同じカタログ名を使用するようにしてください。

#### ライセンスの設定

| 設定キー名 | 必須 | 説明 |
|:---------|:-----|:-----|
| `spark.sql.catalog.<CATALOG_NAME>.license.key` | はい | ScalarDB AnalyticsのライセンスキーのJSON文字列 |
| `spark.sql.catalog.<CATALOG_NAME>.license.cert_pem` | はい | ScalarDB AnalyticsライセンスのPEMエンコードされた証明書の文字列。`cert_pem`または`cert_path`のいずれかを設定する必要があります。 |
| `spark.sql.catalog.<CATALOG_NAME>.license.cert_path` | はい | ScalarDB AnalyticsライセンスのPEMエンコードされた証明書へのパス。`cert_pem`または`cert_path`のいずれかを設定する必要があります。 |

#### データソースの設定

ScalarDB Analyticsは複数のタイプのデータソースをサポートしています。各タイプには特定の設定パラメータが必要です：

<Tabs groupId="data-source" queryString>
  <TabItem value="scalardb" label="ScalarDB">

:::note

ScalarDB AnalyticsはScalarDBをデータソースとしてサポートしています。この表では、ScalarDBをデータソースとして設定する方法について説明します。

:::

| 設定キー名 | 必須 | 説明 |
|:---------|:-----|:-----|
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.type` | はい | 常に`scalardb`を設定します |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.config_path` | はい | ScalarDBの設定ファイルへのパス |

:::tip

`<DATA_SOURCE_NAME>`には任意の名前を使用できます。

:::

  </TabItem>
  <TabItem value="mysql" label="MySQL">

| 設定キー名 | 必須 | 説明 |
|:---------|:-----|:-----|
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.type` | はい | 常に`mysql`を設定します |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.host` | はい | MySQLサーバーのホスト名 |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.port` | はい | MySQLサーバーのポート番号 |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.username` | はい | MySQLサーバーのユーザー名 |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.password` | はい | MySQLサーバーのパスワード |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.database` | いいえ | 接続するデータベースの名前 |

:::tip

`<DATA_SOURCE_NAME>`には任意の名前を使用できます。

:::

  </TabItem>
  <TabItem value="postgresql" label="PostgreSQL">

| 設定キー名 | 必須 | 説明 |
|:---------|:-----|:-----|
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.type` | はい | 常に`postgresql`または`postgres`を設定します |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.host` | はい | PostgreSQLサーバーのホスト名 |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.port` | はい | PostgreSQLサーバーのポート番号 |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.username` | はい | PostgreSQLサーバーのユーザー名 |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.password` | はい | PostgreSQLサーバーのパスワード |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.database` | はい | 接続するデータベースの名前 |

:::tip

`<DATA_SOURCE_NAME>`には任意の名前を使用できます。

:::

  </TabItem>
  <TabItem value="oracle" label="Oracle">

| 設定キー名 | 必須 | 説明 |
|:---------|:-----|:-----|
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.type` | はい | 常に`oracle`を設定します |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.host` | はい | Oracleサーバーのホスト名 |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.port` | はい | Oracleサーバーのポート番号 |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.username` | はい | Oracleサーバーのユーザー名 |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.password` | はい | Oracleサーバーのパスワード |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.service_name` | はい | Oracleサーバーのサービス名 |

:::tip

`<DATA_SOURCE_NAME>`には任意の名前を使用できます。

:::

  </TabItem>
  <TabItem value="sqlserver" label="SQL Server">

| 設定キー名 | 必須 | 説明 |
|:---------|:-----|:-----|
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.type` | はい | 常に`sqlserver`または`mssql`を設定します |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.host` | はい | SQL Serverのホスト名 |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.port` | はい | SQL Serverのポート番号 |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.username` | はい | SQL Serverのユーザー名 |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.password` | はい | SQL Serverのパスワード |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.database` | いいえ | 接続するデータベースの名前 |
| `spark.sql.catalog.<CATALOG_NAME>.data_source.<DATA_SOURCE_NAME>.secure` | いいえ | SQL Serverへの接続にセキュアな接続を使用するかどうか。セキュアな接続を使用する場合は`true`を設定します。 |

:::tip

`<DATA_SOURCE_NAME>`には任意の名前を使用できます。

:::

  </TabItem>
</Tabs>

#### 設定例

以下は、複数のデータソースを持つ`scalardb`という名前のカタログを設定するScalarDB Analyticsの設定例です：

```conf
# Sparkプラグインの設定
spark.jars.packages com.scalar-labs:scalardb-analytics-spark-all-<SPARK_VERSION>_<SCALA_VERSION>:<SCALARDB_ANALYTICS_VERSION>
spark.sql.extensions com.scalar.db.analytics.spark.Extensions
spark.sql.catalog.scalardb com.scalar.db.analytics.spark.ScalarCatalog

# ライセンスの設定
spark.sql.catalog.scalardb.license.key <LICENSE_KEY>
spark.sql.catalog.scalardb.license.cert_pem <LICENSE_PEM_ENCODED_CERTIFICATE>

# データソースの設定
spark.sql.catalog.scalardb.data_source.scalardb.type scalardb
spark.sql.catalog.scalardb.data_source.scalardb.config_path /path/to/scalardb.properties

spark.sql.catalog.scalardb.data_source.mysql_source.type mysql
spark.sql.catalog.scalardb.data_source.mysql_source.host localhost
spark.sql.catalog.scalardb.data_source.mysql_source.port 3306
spark.sql.catalog.scalardb.data_source.mysql_source.username root
spark.sql.catalog.scalardb.data_source.mysql_source.password password
spark.sql.catalog.scalardb.data_source.mysql_source.database mydb
```

括弧内の内容は以下のように変更する必要があります：

- `<LICENSE_KEY>`: ScalarDB Analyticsのライセンスキー
- `<LICENSE_PEM_ENCODED_CERTIFICATE>`: ScalarDB AnalyticsライセンスのPEMエンコードされた証明書
- `<SPARK_VERSION>`: 使用しているSparkのメジャーおよびマイナーバージョン（例：3.4）
- `<SCALA_VERSION>`: Sparkインストールに対応するScalaのメジャーおよびマイナーバージョン（例：2.12または2.13）
- `<SCALARDB_ANALYTICS_VERSION>`: ScalarDB Analyticsのバージョン

### ScalarDB Analytics 依存関係の追加

ScalarDB Analytics は Maven Central Repository でホストされています。パッケージ名は `scalardb-analytics-spark-all-<SPARK_VERSION>_<SCALA_VERSION>:<SCALARDB_ANALYTICS_VERSION>` で、以下の通りです：

- `<SPARK_VERSION>`: 使用している Spark のメジャーおよびマイナーバージョン（例：3.4）
- `<SCALA_VERSION>`: Spark インストールに対応する Scala のメジャーおよびマイナーバージョン（例：2.12 または 2.13）
- `<SCALARDB_ANALYTICS_VERSION>`: ScalarDB Analytics のバージョン

バージョンの互換性の詳細については、[バージョン互換性](#バージョン互換性)を参照してください。

プロジェクトのビルド設定を設定することで、この依存関係をプロジェクトに追加できます。例えば、Gradleを使用している場合は、`build.gradle`ファイルに以下を追加できます：

```groovy
dependencies {
    implementation 'com.scalar-labs:scalardb-analytics-spark-all-<SPARK_VERSION>_<SCALA_VERSION>:<SCALARDB_ANALYTICS_VERSION>'
}
```

:::note

Gradle の Shadow プラグインや Maven の Shade プラグインなどを使用して、アプリケーションを単一の fat JAR ファイルにバンドルする場合は、使用しているプラグインに応じて `provided` や `shadow` などの適切な configuration を選択して、fat JAR ファイルから ScalarDB Analytics を除外する必要があります。

:::

## Spark アプリケーションの開発

このセクションでは、Java を使用して ScalarDB Analytics を使用する Spark アプリケーションを開発する方法について説明します。

ScalarDB Analytics を使用した Spark アプリケーションの開発には3つの方法があります：

1. **Sparkドライバーアプリケーション**: クラスター内で実行される従来のSparkアプリケーション
2. **Spark Connectアプリケーション**: Spark Connectプロトコルを使用するリモートアプリケーション
3. **JDBCアプリケーション**: JDBCインターフェースを使用するリモートアプリケーション

:::note

環境によっては、上記のすべての方法を使用できない場合があります。サポートされる機能とデプロイメントオプションの詳細については、[サポートされるマネージド Spark サービスとそのアプリケーションタイプ](./deployment.mdx#サポートされるマネージド-spark-サービスとそのアプリケーションタイプ)を参照してください。

:::

これらのすべての方法で、同じテーブル識別子形式を使用して ScalarDB Analytics のテーブルを参照できます。ScalarDB Analytics がデータソースからカタログ情報をマッピングする方法の詳細については、[データソース別のカタログ情報マッピング](./design.mdx#データソース別のカタログ情報マッピング)を参照してください。

<Tabs groupId="spark-application-type" queryString>
  <TabItem value="spark-driver" label="Spark ドライバーアプリケーション">

ScalarDB Analyticsには一般的に使用される`SparkSession`クラスを使用できます。また、YARN、Kubernetes、スタンドアロン、ローカルモードなど、Sparkがサポートするあらゆるタイプのクラスターデプロイメントを使用できます。

ScalarDB Analyticsのテーブルからデータを読み取るには、通常のSparkテーブルを読み取る場合と同じように`spark.sql`または`spark.read.table`関数を使用できます。

まず、Javaプロジェクトをセットアップする必要があります。例えば、Gradleを使用している場合は、`build.gradle`ファイルに以下を追加できます：

```groovy
dependencies {
    implementation 'com.scalar-labs:scalardb-analytics-spark-<SPARK_VERSION>_<SCALA_VERSION>:<SCALARDB_ANALYTICS_VERSION>'
}
```

以下はSparkドライバーアプリケーションの例です：

```java
import org.apache.spark.sql.SparkSession;

public class MyApp {
    public static void main(String[] args) {
        // SparkSessionを作成
        try (SparkSession spark = SparkSession.builder().getOrCreate()) {
            // ScalarDB Analyticsのテーブルからデータを読み取る
            spark.sql("SELECT * FROM my_catalog.my_data_source.my_namespace.my_table").show();
        }
    }
}
```

その後、`spark-submit`コマンドを使用してアプリケーションをビルドして実行できます。

:::note

通常のSparkアプリケーションと同様に、アプリケーションのfat JARファイルをビルドする必要がある場合があります。

:::

```console
spark-submit --class MyApp --master local[*] my-spark-application-all.jar
```

:::tip

`spark-sql`や`spark-shell`などのSparkが提供する他のCLIツールを使用して、ScalarDB Analyticsのテーブルを操作することもできます。

:::

  </TabItem>
  <TabItem value="spark-connect" label="Spark Connect アプリケーション">

[Spark Connect](https://spark.apache.org/spark-connect/) を使用して ScalarDB Analytics と対話できます。Spark Connect を使用することで、リモートの Spark クラスターにアクセスし、Spark ドライバーアプリケーションと同じ方法でデータを読み取ることができます。以下では、Spark Connect の使用方法について簡単に説明します。

まず、リモートの Spark クラスターで以下のコマンドを実行して、Spark Connect サーバーを起動する必要があります：

```console
./sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_<SCALA_VERSION>:<SPARK_FULL_VERSION>,com.scalar-labs:scalardb-analytics-spark-all-<SPARK_VERSION>_<SCALA_VERSION>:<SCALARDB_ANALYTICS_VERSION>
```

括弧内の内容は以下のように変更する必要があります：

- `<SCALA_VERSION>`: Spark インストールに対応する Scala のメジャーおよびマイナーバージョン（例：2.12 または 2.13）
- `<SPARK_FULL_VERSION>`: 使用している Spark の完全なバージョン（例：3.5.3）
- `<SPARK_VERSION>`: 使用している Spark のメジャーおよびマイナーバージョン（例：3.5）
- `<SCALARDB_ANALYTICS_VERSION>`: ScalarDB Analytics のバージョン

:::note

パッケージのバージョンは、使用している Spark と ScalarDB Analytics のバージョンと一致する必要があります。

:::

また、アプリケーションに Spark Connect クライアントパッケージを含める必要があります。例えば、Gradle を使用している場合は、`build.gradle` ファイルに以下を追加できます：

```kotlin
implementation("org.apache.spark:spark-connect-client-jvm_2.12:3.5.3")
```

その後、Spark Connect クライアントアプリケーションを作成してサーバーに接続し、データを読み取ることができます。

```java
import org.apache.spark.sql.SparkSession;

public class MyApp {
    public static void main(String[] args) {
        try (SparkSession spark = SparkSession.builder()
            .remote("sc://<CONNECT_SERVER_URL>:<CONNECT_SERVER_PORT>")
            .getOrCreate()) {

            // ScalarDB Analyticsのテーブルからデータを読み取る
            spark.sql("SELECT * FROM my_catalog.my_data_source.my_namespace.my_table").show();
        }
    }
}
```

以下のコマンドを実行して、Spark Connect クライアントアプリケーションを通常の Java アプリケーションとして実行できます：

```console
java -jar my-spark-connect-client.jar
```

Spark Connect の使用方法の詳細については、[Spark Connect のドキュメント](https://spark.apache.org/docs/latest/spark-connect-overview.html)を参照してください。

  </TabItem>
  <TabItem value="jdbc" label="JDBC アプリケーション">

残念ながら、Spark Thrift JDBC サーバーは ScalarDB Analytics に必要な Spark の機能をサポートしていないため、Apache Spark 環境でJDBCを使用して ScalarDB Analytics からデータを読み取ることはできません。JDBC アプリケーションについてここで言及しているのは、一部のマネージド Spark サービスが JDBC インターフェースを介して Spark クラスターと対話する異なる方法を提供しているためです。詳細については、[サポートされているマネージド Spark サービスとそのアプリケーションタイプ](./deployment.mdx#サポートされているマネージド-spark-サービスとそのアプリケーションタイプ)を参照してください。

  </TabItem>
</Tabs>

## カタログ情報のマッピング

ScalarDB Analyticsは、データソース、名前空間、テーブル、列を含む独自のカタログを管理します。この情報は自動的にSparkカタログにマッピングされます。このセクションでは、ScalarDB Analyticsがカタログ情報をSparkカタログにマッピングする方法について説明します。

データソース内の情報がScalarDB Analyticsカタログにマッピングされる方法の詳細については、[データソース別のカタログ情報マッピング](./design.mdx#データソース別のカタログ情報マッピング)を参照してください。

### カタログレベルのマッピング

ScalarDB Analyticsカタログの各カタログレベルオブジェクトは、Sparkカタログにマッピングされます。以下の表は、カタログレベルがどのようにマッピングされるかを示しています：

#### データソーステーブル

ScalarDB Analyticsカタログのデータソースのテーブルは、Sparkテーブルにマッピングされます。ScalarDB Analyticsテーブルに対応するSparkテーブルの識別子には以下の形式が使用されます：

```console
<CATALOG_NAME>.<DATA_SOURCE_NAME>.<NAMESPACE_NAMES>.<TABLE_NAME>
```

括弧内の内容は以下の通りです：

- `<CATALOG_NAME>`: カタログの名前
- `<DATA_SOURCE_NAME>`: データソースの名前
- `<NAMESPACE_NAMES>`: 名前空間の名前。名前空間が複数レベルある場合は、ドット（`.`）で区切って連結されます
- `<TABLE_NAME>`: テーブルの名前

例えば、`my_catalog` という名前の ScalarDB カタログに、`my_data_source` という名前のデータソースと `my_schema` という名前のスキーマがある場合、そのスキーマ内の `my_table` という名前のテーブルを `my_catalog.my_data_source.my_schema.my_table` として参照できます。

#### ビュー

ScalarDB Analytics のビューは、ビューではなく Spark カタログのテーブルとして提供されます。ScalarDB Analytics ビューに対応する Spark テーブルの識別子には以下の形式が使用されます：

```console
<CATALOG_NAME>.view.<VIEW_NAMESPACE_NAMES>.<VIEW_NAME>
```

括弧内の内容は以下の通りです：

- `<CATALOG_NAME>`: カタログの名前
- `<VIEW_NAMESPACE_NAMES>`: ビュー名前空間の名前。ビュー名前空間が複数レベルある場合は、ドット（`.`）で区切って連結されます
- `<VIEW_NAME>`: ビューの名前

例えば、`my_catalog` という名前の ScalarDB カタログと `my_view_namespace` という名前のビュー名前空間がある場合、その名前空間内の `my_view` という名前のビューを `my_catalog.view.my_view_namespace.my_view` として参照できます。

:::note

データソーステーブル識別子との競合を避けるため、`view` が接頭辞として付けられます。

:::

##### WAL 解釈ビュー

[ScalarDB Analytics の設計](./design.mdx)で説明されているように、ScalarDB Analytics は WAL 解釈ビューと呼ばれる特別なタイプのビューを提供します。これらのビューは、ScalarDB データソースのテーブルに対して自動的に作成され、テーブル内の WAL メタデータを解釈することでユーザーフレンドリーなビューを提供します。

元の ScalarDB テーブルのデータソース名と名前空間の名前が WAL 解釈ビューのビュー名前空間の名前として使用されるため、`my_data_source` という名前のデータソースの `my_namespace` という名前の名前空間にある `my_table` という名前の ScalarDB テーブルがある場合、そのテーブルの WAL 解釈ビューを `my_catalog.view.my_data_source.my_namespace.my_table` として参照できます。

### データ型マッピング

ScalarDB Analytics は、カタログ内のデータ型を Spark データ型にマッピングします。以下の表は、データ型がどのようにマッピングされるかを示しています：

| ScalarDB データ型 | Spark データ型      |
|:----------------|:-------------------|
| `BYTE`          | `Byte`             |
| `SMALLINT`      | `Short`            |
| `INT`           | `Integer`          |
| `BIGINT`        | `Long`             |
| `FLOAT`         | `Float`            |
| `DOUBLE`        | `Double`           |
| `DECIMAL`       | `Decimal`          |
| `TEXT`          | `String`           |
| `BLOB`          | `Binary`           |
| `BOOLEAN`       | `Boolean`          |
| `DATE`          | `Date`             |
| `TIME`          | `TimestampNTZ`     |
| `TIMESTAMP`     | `TimestampNTZ`     |
| `TIMESTAMPTZ`   | `Timestamp`        |
| `DURATION`      | `CalendarInterval` |
| `INTERVAL`      | `CalendarInterval` |

## バージョン互換性

Spark と Scala は異なるマイナーバージョン間で互換性がない場合があるため、ScalarDB Analytics は様々な Spark と Scala バージョン向けに `scalardb-analytics-spark-all-<SPARK_VERSION>_<SCALA_VERSION>` という形式で異なるアーティファクトを提供しています。使用している Spark と Scala のバージョンに合わせてアーティファクトを選択してください。例えば、Spark 3.5 と Scala 2.13 を使用している場合は、`scalardb-analytics-spark-all-3.5_2.13` を指定する必要があります。

Java バージョンに関しては、ScalarDB Analytics は Java 8以降をサポートしています。

以下は、ScalarDB Analytics の各バージョンでサポートされている Spark と Scalar のバージョンのリストです。

| ScalarDB Analytics バージョン | ScalarDB バージョン | サポートされている Spark バージョン | サポートされている Scala バージョン | 最小 Java バージョン |
|:---------------------------|:-----------------|:-------------------------|:-------------------------|:---------------------|
| 3.15                       | 3.15             | 3.5, 3.4                 | 2.13, 2.12               | 8                    |
| 3.14                       | 3.14             | 3.5, 3.4                 | 2.13, 2.12               | 8                    |
| 3.12                       | 3.12             | 3.5, 3.4                 | 2.13, 2.12               | 8                    |

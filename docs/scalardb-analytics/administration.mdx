---
tags:
  - Enterprise Option
displayed_sidebar: docsEnglish
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Manage ScalarDB Analytics

import WarningLicenseKeyContact from "/src/components/en-us/_warning-license-key-contact.mdx";

This guide explains how to set up and manage the ScalarDB Analytics server and its catalogs. The ScalarDB Analytics server is the implementation of the Universal Data Catalog described in the [ScalarDB Analytics Design](design.mdx), providing centralized metadata management for analytical queries across multiple databases.

<WarningLicenseKeyContact product="ScalarDB Analytics" />

## Overview

ScalarDB Analytics provides a universal data catalog that enables unified access to multiple databases through a single interface. Based on the Universal Data Catalog architecture described in the [ScalarDB Analytics Design](design.mdx#universal-data-catalog) documentation, the system consists of two main components:

1. **ScalarDB Analytics server**: A gRPC-based service that manages:
   - **Catalog metadata**: Organizes data sources, namespaces, tables, and columns
   - **Data source connections**: Maintains connection information and credentials for external databases
   - **License validation**: Verifies enterprise licenses
   - **Usage metering**: Tracks resource usage for billing purposes

   The server provides two gRPC endpoints:
   - Port 11051: Catalog service for metadata operations
   - Port 11052: Metering service for usage tracking

2. **ScalarDB Analytics CLI**: A command-line tool that communicates with the server to manage catalogs, register data sources, and perform administrative tasks

## Setup

Before managing catalogs, you need to set up and configure the ScalarDB Analytics server and CLI.

### Server setup

#### Prerequisites: Metadata database

The server requires a database to store catalog metadata and data source connection information. We refer to this database as the **metadata database** throughout this documentation. ScalarDB Analytics supports the following databases for the metadata database:

- PostgreSQL
- MySQL
- SQL Server
- Oracle

Create a database and user with appropriate privileges before starting the server. The specific commands vary by database type.

#### Server configuration

Create a server configuration file (e.g., `scalardb-analytics-server.properties`). The following example uses PostgreSQL as the metadata database:

```properties
# Metadata database configuration (required)
scalar.db.analytics.server.db.url=jdbc:postgresql://localhost:5432/scalardb_analytics
scalar.db.analytics.server.db.username=analytics_user
scalar.db.analytics.server.db.password=your_secure_password

# gRPC server configuration (optional)
scalar.db.analytics.server.catalog.port=11051  # default
scalar.db.analytics.server.metering.port=11052  # default

# TLS configuration (optional but recommended for production)
scalar.db.analytics.server.tls.enabled=true
scalar.db.analytics.server.tls.cert_chain_path=/path/to/server.crt
scalar.db.analytics.server.tls.private_key_path=/path/to/server.key

# License configuration (required)
scalar.db.analytics.server.licensing.license_key=<YOUR_LICENSE_KEY>
scalar.db.analytics.server.licensing.license_check_cert_pem=<YOUR_LICENSE_CERT_PEM>

# Metering storage configuration (required)
scalar.db.analytics.server.metering.storage.provider=filesystem
scalar.db.analytics.server.metering.storage.path=/var/scalardb-analytics/metering
```

For detailed configuration options, see the [Configuration reference](configuration.mdx).

#### Starting the server

Start the ScalarDB Analytics server with your configuration:

```console
docker run -d \
  --name scalardb-analytics-server \
  -p 11051:11051 \
  -p 11052:11052 \
  -v /path/to/scalardb-analytics-server.properties:/scalardb-analytics-server/server.properties \
  ghcr.io/scalar-labs/scalardb-analytics-server:<VERSION>
```

Replace `<VERSION>` with the ScalarDB Analytics version you want to use. You can find available versions at the [Docker registry page](https://github.com/scalar-labs/scalardb-analytics/pkgs/container/scalardb-analytics-server).

The container uses the configuration file at `/scalardb-analytics-server/server.properties` by default.

The server will perform the following during startup:

1. Validate the license
2. Connect to the metadata database
3. Start gRPC services on the configured ports
4. Begin accepting client connections

#### Health checks (optional)

If you want to verify the server is running properly, you can use grpc-health-probe (included in the Docker container):

```console
# Check catalog service health
docker exec scalardb-analytics-server grpc-health-probe -addr=localhost:11051

# Check metering service health
docker exec scalardb-analytics-server grpc-health-probe -addr=localhost:11052

# For TLS-enabled servers
docker exec scalardb-analytics-server grpc-health-probe -addr=localhost:11051 -tls -tls-ca-cert=/path/to/ca.crt
```

### CLI setup

#### Installing the CLI

The `scalardb-analytics-cli` tool is available as a Docker image:

```console
# Pull the CLI image
docker pull ghcr.io/scalar-labs/scalardb-analytics-cli:<VERSION>
```

Replace `<VERSION>` with the ScalarDB Analytics version you want to use. Available versions can be found at the [Docker registry page](https://github.com/scalar-labs/scalardb-analytics/pkgs/container/scalardb-analytics-cli).

To run CLI commands, you'll need to mount your configuration file into the container:

```console
# Example: List catalogs
docker run --rm \
  -v $(pwd)/client.properties:/config/client.properties:ro \
  ghcr.io/scalar-labs/scalardb-analytics-cli:<VERSION> \
  -c /config/client.properties \
  catalog list
```

#### Client configuration

Create a configuration file named `client.properties` in your current directory:

```properties
# Server connection
scalar.db.analytics.client.server.host=localhost
scalar.db.analytics.client.server.catalog.port=11051
scalar.db.analytics.client.server.metering.port=11052

# TLS/SSL configuration (if enabled on server)
scalar.db.analytics.client.server.tls.enabled=true
scalar.db.analytics.client.server.tls.ca_root_cert_path=/path/to/ca.crt
scalar.db.analytics.client.server.tls.override_authority=analytics.example.com
```

For detailed configuration options, see the [Configuration reference](configuration.mdx).

#### Setting up an alias (optional)

For convenience, you can create an alias to avoid typing the long Docker command each time:

```console
alias scalardb-analytics-cli='docker run --rm -v $(pwd)/client.properties:/config/client.properties:ro ghcr.io/scalar-labs/scalardb-analytics-cli:<VERSION> -c /config/client.properties'
```

With this alias, you can run commands more simply:

```console
scalardb-analytics-cli catalog list
```

## CLI command reference

The ScalarDB Analytics CLI uses a hierarchical command structure:

```
scalardb-analytics <resource> <operation> [options]
```

Available resources:

- **catalog**: Top-level containers for organizing data sources
- **data-source**: External databases registered within catalogs
- **namespace**: Database-specific organizational units (auto-discovered)
- **table**: Data structures within namespaces (auto-discovered)

Note: In all examples below, we assume you're using the Docker alias created earlier. If running Docker commands directly, replace `scalardb-analytics-cli` with the full Docker command.

### Catalog operations

```console
# Create a new catalog
scalardb-analytics-cli catalog create --catalog <catalog-name>

# List all catalogs
scalardb-analytics-cli catalog list

# Show catalog details (by name or ID)
scalardb-analytics-cli catalog describe --catalog <catalog-name>
scalardb-analytics-cli catalog describe --catalog-id <catalog-uuid>

# Delete a catalog (fails if not empty unless --cascade is used)
scalardb-analytics-cli catalog delete --catalog <catalog-name>
scalardb-analytics-cli catalog delete --catalog <catalog-name> --cascade
```

### Data source operations

```console
# Register a new data source using a JSON definition file
scalardb-analytics-cli data-source register --data-source-json <path-to-json>

# List all data sources in a catalog
scalardb-analytics-cli data-source list --catalog <catalog-name>

# Show data source details (by name or ID)
scalardb-analytics-cli data-source describe --catalog <catalog-name> --data-source <data-source-name>
scalardb-analytics-cli data-source describe --data-source-id <data-source-uuid>

# Delete a data source (fails if not empty unless --cascade is used)
scalardb-analytics-cli data-source delete --catalog <catalog-name> --data-source <data-source-name>
scalardb-analytics-cli data-source delete --catalog <catalog-name> --data-source <data-source-name> --cascade
```

The `register` command requires a JSON definition file. The JSON file format is described in the [Data source configuration](#data-source-configuration) section below.

### Namespace operations

```console
# List all namespaces in a data source
scalardb-analytics-cli namespace list --catalog <catalog-name> --data-source <data-source-name>

# Show namespace details (by name or ID)
# For nested namespaces, use '.' as a separator (e.g., --namespace parent.child)
scalardb-analytics-cli namespace describe --catalog <catalog-name> --data-source <data-source-name> --namespace <namespace-name>
scalardb-analytics-cli namespace describe --namespace-id <namespace-uuid>
```

### Table operations

```console
# List all tables in a namespace
scalardb-analytics-cli table list --catalog <catalog-name> --data-source <data-source-name> --namespace <namespace-name>

# Show table schema with all columns (by name or ID)
# For nested namespaces, use '.' as a separator (e.g., --namespace parent.child)
scalardb-analytics-cli table describe --catalog <catalog-name> --data-source <data-source-name> --namespace <namespace-name> --table <table-name>
scalardb-analytics-cli table describe --table-id <table-uuid>
```

## Data source configuration

### Data source JSON format

Data sources are registered using JSON definition files with the following structure:

```json
{
  "catalog": "<catalog-name>", // The catalog to register the data source in
  "name": "<data-source-name>", // A unique name for this data source
  "type": "<database-type>", // Database type: postgres, mysql, scalardb, sqlserver, oracle, dynamodb
  "provider": {
    // Type-specific connection configuration
    // Configuration varies by database type
  }
}
```

The `provider` section contains database-specific connection settings that vary based on the `type` field.

### Provider configuration by type

The following sections show the provider configuration for each supported database type:

<Tabs groupId="data-source-type" queryString>
  <TabItem value="scalardb" label="ScalarDB" default>

#### Configuration

| Field        | Required | Description                             | Default |
| ------------ | -------- | --------------------------------------- | ------- |
| `configPath` | Yes      | Path to the ScalarDB configuration file | -       |

**Example:**

```json
{
  "catalog": "production",
  "name": "scalardb_source",
  "type": "scalardb",
  "provider": {
    "configPath": "/path/to/scalardb.properties"
  }
}
```

  </TabItem>
  <TabItem value="postgresql" label="PostgreSQL">

#### Configuration

| Field      | Required | Description                 | Default |
| ---------- | -------- | --------------------------- | ------- |
| `host`     | Yes      | PostgreSQL server hostname  | -       |
| `port`     | Yes      | Port number                 | -       |
| `username` | Yes      | Database user               | -       |
| `password` | Yes      | Database password           | -       |
| `database` | Yes      | Database name to connect to | -       |

**Example:**

```json
{
  "catalog": "production",
  "name": "postgres_customers",
  "type": "postgres",
  "provider": {
    "host": "postgres.example.com",
    "port": 5432,
    "username": "analytics_user",
    "password": "secure_password",
    "database": "customers"
  }
}
```

  </TabItem>
  <TabItem value="mysql" label="MySQL">

#### Configuration

| Field      | Required | Description                                                             | Default |
| ---------- | -------- | ----------------------------------------------------------------------- | ------- |
| `host`     | Yes      | MySQL server hostname                                                   | -       |
| `port`     | Yes      | Port number                                                             | -       |
| `username` | Yes      | Database user                                                           | -       |
| `password` | Yes      | Database password                                                       | -       |
| `database` | No       | Specific database to import. If omitted, all databases will be imported | -       |

**Example:**

```json
{
  "catalog": "production",
  "name": "mysql_orders",
  "type": "mysql",
  "provider": {
    "host": "mysql.example.com",
    "port": 3306,
    "username": "analytics_user",
    "password": "secure_password",
    "database": "orders" // Optional - if omitted, all databases will be imported
  }
}
```

  </TabItem>
  <TabItem value="oracle" label="Oracle">

#### Configuration

| Field         | Required | Description            | Default |
| ------------- | -------- | ---------------------- | ------- |
| `host`        | Yes      | Oracle server hostname | -       |
| `port`        | Yes      | Port number            | -       |
| `username`    | Yes      | Database user          | -       |
| `password`    | Yes      | Database password      | -       |
| `serviceName` | Yes      | Oracle service name    | -       |

**Example:**

```json
{
  "catalog": "production",
  "name": "oracle_warehouse",
  "type": "oracle",
  "provider": {
    "host": "oracle.example.com",
    "port": 1521,
    "username": "analytics_user",
    "password": "secure_password",
    "serviceName": "ORCL"
  }
}
```

  </TabItem>
  <TabItem value="sql-server" label="SQL Server">

#### Configuration

| Field      | Required | Description                     | Default |
| ---------- | -------- | ------------------------------- | ------- |
| `host`     | Yes      | SQL Server hostname             | -       |
| `port`     | Yes      | Port number                     | -       |
| `username` | Yes      | Database user                   | -       |
| `password` | Yes      | Database password               | -       |
| `database` | No       | Specific database to connect to | -       |
| `secure`   | No       | Enable encryption               | -       |

**Example:**

```json
{
  "catalog": "production",
  "name": "sqlserver_analytics",
  "type": "sqlserver",
  "provider": {
    "host": "sqlserver.example.com",
    "port": 1433,
    "username": "sa",
    "password": "secure_password",
    "database": "analytics", // Optional - if specified, only this database will be imported
    "secure": true // Optional - enable encryption
  }
}
```

  </TabItem>
  <TabItem value="dynamodb" label="DynamoDB">

#### Configuration

| Field      | Required | Description                  | Default |
| ---------- | -------- | ---------------------------- | ------- |
| `region`   | Yes\*    | AWS region (e.g., us-east-1) | -       |
| `endpoint` | Yes\*    | Custom endpoint URL          | -       |
| `schema`   | Yes      | Complete schema definition   | -       |

\* Either `region` or `endpoint` must be specified (not both).

Since DynamoDB is schema-less, you must provide a complete schema definition.

##### Schema structure

| Field                                              | Required | Description                            | Default |
| -------------------------------------------------- | -------- | -------------------------------------- | ------- |
| `.schema`                                          | Yes      | Complete schema definition             | -       |
| `.schema.namespaces[]`                             | Yes      | Array of namespace definitions         | -       |
| `.schema.namespaces[].names[]`                     | Yes      | Array of namespace names (strings)     | -       |
| `.schema.namespaces[].tables[]`                    | Yes      | Array of table definitions             | -       |
| `.schema.namespaces[].tables[].name`               | Yes      | Table name                             | -       |
| `.schema.namespaces[].tables[].columns[]`          | Yes      | Array of column definitions            | -       |
| `.schema.namespaces[].tables[].columns[].name`     | Yes      | Column name                            | -       |
| `.schema.namespaces[].tables[].columns[].type`     | Yes      | Data type                              | -       |
| `.schema.namespaces[].tables[].columns[].nullable` | No       | Whether column can contain null values | true    |

**Example:**

```json
{
  "catalog": "production",
  "name": "dynamodb_events",
  "type": "dynamodb",
  "provider": {
    "region": "us-east-1",
    "schema": {
      "namespaces": [
        {
          "names": ["production"],
          "tables": [
            {
              "name": "user_events",
              "columns": [
                { "name": "user_id", "type": "TEXT", "nullable": false },
                {
                  "name": "event_time",
                  "type": "TIMESTAMP",
                  "nullable": false
                },
                { "name": "event_type", "type": "TEXT" },
                { "name": "event_data", "type": "TEXT" }
              ]
            }
          ]
        }
      ]
    }
  }
}
```

  </TabItem>
</Tabs>

## Catalog metadata reference

### Catalog structure mappings by data source

When registering a data source to ScalarDB Analytics, the catalog structure of the data source, that is, namespaces, tables, and columns, are resolved and registered to the universal data catalog. To resolve the catalog structure of the data source, a particular object on the data sources side are mapped to the universal data catalog object.

#### Catalog-level mappings

The catalog-level mappings are the mappings of the namespace names, table names, and column names from the data sources to the universal data catalog. To see the catalog-level mappings in each data source, select a data source.

<Tabs groupId="data-source" queryString>
  <TabItem value="scalardb" label="ScalarDB" default>
    The catalog structure of ScalarDB is automatically resolved by ScalarDB Analytics. The catalog-level objects are mapped as follows:

    - The ScalarDB namespace is mapped to the namespace. Therefore, the namespace of the ScalarDB data source is always single level, consisting of only the namespace name.
    - The ScalarDB table is mapped to the table.
    - The ScalarDB column is mapped to the column.

  </TabItem>

  <TabItem value="postgresql" label="PostgreSQL" default>
    The catalog structure of PostgreSQL is automatically resolved by ScalarDB Analytics. The catalog-level objects are mapped as follows:

    - The PostgreSQL schema is mapped to the namespace. Therefore, the namespace of the PostgreSQL data source is always single level, consisting of only the schema name.
        - Only user-defined schemas are mapped to namespaces. The following system schemas are ignored:
          - `information_schema`
          - `pg_catalog`
    - The PostgreSQL table is mapped to the table.
    - The PostgreSQL column is mapped to the column.

  </TabItem>
  <TabItem value="mysql" label="MySQL">
    The catalog structure of MySQL is automatically resolved by ScalarDB Analytics. The catalog-level objects are mapped as follows:

    - The MySQL database is mapped to the namespace. Therefore, the namespace of the MySQL data source is always single level, consisting of only the database name.
        - Only user-defined databases are mapped to namespaces. The following system databases are ignored:
            - `mysql`
            - `sys`
            - `information_schema`
            - `performance_schema`
    - The MySQL table is mapped to the table.
    - The MySQL column is mapped to the column.

  </TabItem>
  <TabItem value="oracle" label="Oracle">
    The catalog structure of Oracle is automatically resolved by ScalarDB Analytics. The catalog-level objects are mapped as follows:

    - The Oracle schema is mapped to the namespace. Therefore, the namespace of the Oracle data source is always single level, consisting of only schema name.
        - Only user-defined schemas are mapped to namespaces. The following system schemas are ignored:
            - `ANONYMOUS`
            - `APPQOSSYS`
            - `AUDSYS`
            - `CTXSYS`
            - `DBSNMP`
            - `DGPDB_INT`
            - `DBSFWUSER`
            - `DVF`
            - `DVSYS`
            - `GGSYS`
            - `GSMADMIN_INTERNAL`
            - `GSMCATUSER`
            - `GSMROOTUSER`
            - `GSMUSER`
            - `LBACSYS`
            - `MDSYS`
            - `OJVMSYS`
            - `ORDDATA`
            - `ORDPLUGINS`
            - `ORDSYS`
            - `OUTLN`
            - `REMOTE_SCHEDULER_AGENT`
            - `SI_INFORMTN_SCHEMA`
            - `SYS`
            - `SYS$UMF`
            - `SYSBACKUP`
            - `SYSDG`
            - `SYSKM`
            - `SYSRAC`
            - `SYSTEM`
            - `WMSYS`
            - `XDB`
            - `DIP`
            - `MDDATA`
            - `ORACLE_OCM`
            - `XS$NULL`

  </TabItem>
  <TabItem value="sql-server" label="SQL Server">
    The catalog structure of SQL Server is automatically resolved by ScalarDB Analytics. The catalog-level objects are mapped as follows:

    - The SQL Server database and schema are mapped to the namespace together. Therefore, the namespace of the SQL Server data source is always two-level, consisting of the database name and the schema name.
        - Only user-defined databases are mapped to namespaces. The following system databases are ignored:
            - `sys`
            - `guest`
            - `INFORMATION_SCHEMA`
            - `db_accessadmin`
            - `db_backupoperator`
            - `db_datareader`
            - `db_datawriter`
            - `db_ddladmin`
            - `db_denydatareader`
            - `db_denydatawriter`
            - `db_owner`
            - `db_securityadmin`
        - Only user-defined schemas are mapped to namespaces. The following system schemas are ignored:
            - `master`
            - `model`
            - `msdb`
            - `tempdb`
    - The SQL Server table is mapped to the table.
    - The SQL Server column is mapped to the column.

  </TabItem>
  <TabItem value="dynamodb" label="DynamoDB">
    Since DynamoDB is schema-less, you need to specify the catalog structure explicitly when registering a DynamoDB data source by using the following format JSON:

    ```json
    {
        "namespaces": [
            {
                "name": "<NAMESPACE_NAME>",
                "tables": [
                    {
                        "name": "<TABLE_NAME>",
                        "columns": [
                            {
                                "name": "<COLUMN_NAME>",
                                "type": "<COLUMN_TYPE>"
                            },
                            ...
                        ]
                    },
                    ...
                ]
            },
            ...
        ]
    }
    ```

    In the specified JSON, you can use any arbitrary namespace names, but the table names must match the table names in DynamoDB and column name and type must match field names and types in DynamoDB.

  </TabItem>
</Tabs>

### Data type mappings

The following sections show how native types from each data source are mapped to ScalarDB Analytics types:

<Tabs groupId="data-source-type" queryString>
  <TabItem value="scalardb" label="ScalarDB" default>

| **ScalarDB Data Type** | **ScalarDB Analytics Data Type** |
| :--------------------- | :------------------------------- |
| `BOOLEAN`              | `BOOLEAN`                        |
| `INT`                  | `INT`                            |
| `BIGINT`               | `BIGINT`                         |
| `FLOAT`                | `FLOAT`                          |
| `DOUBLE`               | `DOUBLE`                         |
| `TEXT`                 | `TEXT`                           |
| `BLOB`                 | `BLOB`                           |
| `DATE`                 | `DATE`                           |
| `TIME`                 | `TIME`                           |
| `TIMESTAMP`            | `TIMESTAMP`                      |
| `TIMESTAMPTZ`          | `TIMESTAMPTZ`                    |

  </TabItem>
  <TabItem value="postgresql" label="PostgreSQL">

| **PostgreSQL Data Type**      | **ScalarDB Analytics Data Type** |
| :---------------------------- | :------------------------------- |
| `integer`                     | `INT`                            |
| `bigint`                      | `BIGINT`                         |
| `real`                        | `FLOAT`                          |
| `double precision`            | `DOUBLE`                         |
| `smallserial`                 | `SMALLINT`                       |
| `serial`                      | `INT`                            |
| `bigserial`                   | `BIGINT`                         |
| `char`                        | `TEXT`                           |
| `varchar`                     | `TEXT`                           |
| `text`                        | `TEXT`                           |
| `bpchar`                      | `TEXT`                           |
| `boolean`                     | `BOOLEAN`                        |
| `bytea`                       | `BLOB`                           |
| `date`                        | `DATE`                           |
| `time`                        | `TIME`                           |
| `time with time zone`         | `TIME`                           |
| `time without time zone`      | `TIME`                           |
| `timestamp`                   | `TIMESTAMP`                      |
| `timestamp with time zone`    | `TIMESTAMPTZ`                    |
| `timestamp without time zone` | `TIMESTAMP`                      |

  </TabItem>
  <TabItem value="mysql" label="MySQL">

| **MySQL Data Type**  | **ScalarDB Analytics Data Type** |
| :------------------- | :------------------------------- |
| `bit`                | `BOOLEAN`                        |
| `bit(1)`             | `BOOLEAN`                        |
| `bit(x)` if _x >= 2_ | `BLOB`                           |
| `tinyint`            | `SMALLINT`                       |
| `tinyint(1)`         | `BOOLEAN`                        |
| `boolean`            | `BOOLEAN`                        |
| `smallint`           | `SMALLINT`                       |
| `smallint unsigned`  | `INT`                            |
| `mediumint`          | `INT`                            |
| `mediumint unsigned` | `INT`                            |
| `int`                | `INT`                            |
| `int unsigned`       | `BIGINT`                         |
| `bigint`             | `BIGINT`                         |
| `float`              | `FLOAT`                          |
| `double`             | `DOUBLE`                         |
| `real`               | `DOUBLE`                         |
| `char`               | `TEXT`                           |
| `varchar`            | `TEXT`                           |
| `text`               | `TEXT`                           |
| `binary`             | `BLOB`                           |
| `varbinary`          | `BLOB`                           |
| `blob`               | `BLOB`                           |
| `date`               | `DATE`                           |
| `time`               | `TIME`                           |
| `datetime`           | `TIMESTAMP`                      |
| `timestamp`          | `TIMESTAMPTZ`                    |

  </TabItem>
  <TabItem value="oracle" label="Oracle">

| **Oracle Data Type**             | **ScalarDB Analytics Data Type** |
| :------------------------------- | :------------------------------- |
| `NUMBER` if _scale = 0_          | `BIGINT`                         |
| `NUMBER` if _scale > 0_          | `DOUBLE`                         |
| `FLOAT` if _precision ≤ 53_      | `DOUBLE`                         |
| `BINARY_FLOAT`                   | `FLOAT`                          |
| `BINARY_DOUBLE`                  | `DOUBLE`                         |
| `CHAR`                           | `TEXT`                           |
| `NCHAR`                          | `TEXT`                           |
| `VARCHAR2`                       | `TEXT`                           |
| `NVARCHAR2`                      | `TEXT`                           |
| `CLOB`                           | `TEXT`                           |
| `NCLOB`                          | `TEXT`                           |
| `BLOB`                           | `BLOB`                           |
| `BOOLEAN`                        | `BOOLEAN`                        |
| `DATE`                           | `DATE`                           |
| `TIMESTAMP`                      | `TIMESTAMPTZ`                    |
| `TIMESTAMP WITH TIME ZONE`       | `TIMESTAMPTZ`                    |
| `TIMESTAMP WITH LOCAL TIME ZONE` | `TIMESTAMP`                      |
| `RAW`                            | `BLOB`                           |

  </TabItem>
  <TabItem value="sql-server" label="SQL Server">

| **SQL Server Data Type** | **ScalarDB Analytics Data Type** |
| :----------------------- | :------------------------------- |
| `bit`                    | `BOOLEAN`                        |
| `tinyint`                | `SMALLINT`                       |
| `smallint`               | `SMALLINT`                       |
| `int`                    | `INT`                            |
| `bigint`                 | `BIGINT`                         |
| `real`                   | `FLOAT`                          |
| `float`                  | `DOUBLE`                         |
| `float(n)` if _n ≤ 24_   | `FLOAT`                          |
| `float(n)` if _n ≥ 25_   | `DOUBLE`                         |
| `binary`                 | `BLOB`                           |
| `varbinary`              | `BLOB`                           |
| `char`                   | `TEXT`                           |
| `varchar`                | `TEXT`                           |
| `nchar`                  | `TEXT`                           |
| `nvarchar`               | `TEXT`                           |
| `ntext`                  | `TEXT`                           |
| `text`                   | `TEXT`                           |
| `date`                   | `DATE`                           |
| `time`                   | `TIME`                           |
| `datetime`               | `TIMESTAMP`                      |
| `datetime2`              | `TIMESTAMP`                      |
| `smalldatetime`          | `TIMESTAMP`                      |
| `datetimeoffset`         | `TIMESTAMPTZ`                    |

  </TabItem>
  <TabItem value="dynamodb" label="DynamoDB">

| **DynamoDB Data Type** | **ScalarDB Analytics Data Type** |
| :--------------------- | :------------------------------- |
| `Number`               | `BYTE`                           |
| `Number`               | `SMALLINT`                       |
| `Number`               | `INT`                            |
| `Number`               | `BIGINT`                         |
| `Number`               | `FLOAT`                          |
| `Number`               | `DOUBLE`                         |
| `Number`               | `DECIMAL`                        |
| `String`               | `TEXT`                           |
| `Binary`               | `BLOB`                           |
| `Boolean`              | `BOOLEAN`                        |

:::warning

It is important to ensure that the field values of `Number` types are parsable as a specified data type for ScalarDB Analytics. For example, if a column that corresponds to a `Number`-type field is specified as an `INT` type, its value must be an integer. If the value is not an integer, an error will occur when running a query.

:::

  </TabItem>
</Tabs>

## Next steps

After setting up the ScalarDB Analytics server and managing catalogs:

1. Configure Spark or other query engines to use your catalogs - see [Configuration Reference](configuration.mdx)
2. Start running analytical queries - see [Run Analytical Queries](run-analytical-queries.mdx)
3. Set up production deployment - see [Deployment Guide](deployment.mdx)
